{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01_data_preparation.ipynb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from openai import OpenAI\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from bertopic import BERTopic\n",
    "import jieba  # For Chinese word segmentation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "import yaml\n",
    "from itables import show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the directory of the current notebook\n",
    "current_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "# Parent directory is the project root\n",
    "PROJECT_ROOT = os.path.dirname(current_dir)\n",
    "print(f\"Project root identified as: {PROJECT_ROOT}\")\n",
    "\n",
    "# Set paths - CONFIGURABLE PARAMETERS\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")\n",
    "DATA_SUBFOLDER = \"data2\"  # Change this to the specific subfolder where your data is located\n",
    "DATA_FILENAME = \"data.csv\"  # Change this to your data filename\n",
    "\n",
    "# Construct full path to the data file\n",
    "DATA_PATH = os.path.join(DATA_DIR, DATA_SUBFOLDER, DATA_FILENAME)\n",
    "print(f\"Looking for data at: {DATA_PATH}\")\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f\"Loaded dataset with shape: {df.shape}\")\n",
    "\n",
    "# ensure the text fields are strings\n",
    "df['english_text'] = df['english_text'].astype(str)\n",
    "df['chinese_text'] = df['chinese_text'].astype(str)\n",
    "df['technique'] = df['technique'].astype(str)\n",
    "df['intent'] = df['intent'].astype(str)\n",
    "\n",
    "\n",
    "# Display basic information\n",
    "df.info()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secrets_path = os.path.join(PROJECT_ROOT, \"secrets.yaml\")\n",
    "with open(secrets_path, 'r') as f:\n",
    "    secrets = yaml.safe_load(f)\n",
    "\n",
    "if \"gpt_key\" not in secrets:\n",
    "    raise ValueError(\"GPT key not found in secrets.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count non-null values per column\n",
    "print(\"\\nNon-null counts:\")\n",
    "print(df.count())\n",
    "\n",
    "# Check for unique values in categorical columns\n",
    "for col in ['success', 'technique', 'intent']:\n",
    "    if df[col].dtype == 'object' or df[col].nunique() < 10:\n",
    "        print(f\"\\nUnique values in {col}:\")\n",
    "        print(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length distribution\n",
    "df['english_text_length'] = df['english_text'].apply(lambda x: len(str(x)) if not pd.isna(x) else 0)\n",
    "df['chinese_text_length'] = df['chinese_text'].apply(lambda x: len(str(x)) if not pd.isna(x) else 0)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['english_text_length'])\n",
    "plt.title('English Text Length Distribution')\n",
    "plt.xlabel('Character Count')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df['chinese_text_length'])\n",
    "plt.title('Chinese Text Length Distribution')\n",
    "plt.xlabel('Character Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "def translate_to_chinese(text):\n",
    "    try:\n",
    "        if text is None or text.strip() == '':\n",
    "            return None\n",
    "        translation = GoogleTranslator(source='en', target='zh-CN').translate(text)\n",
    "        return translation\n",
    "    except Exception as e:\n",
    "        print(f\"Error translating text: {text}, Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Apply translation function to each row in the DataFrame\n",
    "df[\"chinese_translation_google\"] = df[\"english_text\"].apply(translate_to_chinese)\n",
    "\n",
    "show(df[[\"english_text\", \"chinese_translation_google\", \"chinese_text\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_to_english(text):\n",
    "    try:\n",
    "        if text is None or text.strip() == '':\n",
    "            return None\n",
    "        translation = GoogleTranslator(source='zh-CN', target='en').translate(text)\n",
    "        return translation\n",
    "    except Exception as e:\n",
    "        print(f\"Error translating text: {text}, Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Apply translation function to each row in the DataFrame\n",
    "df[\"english_translation_google\"] = df[\"chinese_text\"].apply(translate_to_english)\n",
    "\n",
    "show(df[[\"chinese_text\", \"english_translation_google\", \"english_text\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(df, text_columns=['english_text', 'chinese_text', 'english_translation_google'], model=\"text-embedding-3-small\"):\n",
    "    \"\"\"Generate OpenAI embeddings for specified text columns.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame containing the text columns\n",
    "        text_columns (list): List of column names to generate embeddings for\n",
    "        model (str): OpenAI embedding model to use\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Original DataFrame with added embedding columns\n",
    "    \"\"\"\n",
    "    # Initialize OpenAI client with API key from secrets\n",
    "    client = OpenAI(api_key=secrets[\"gpt_key\"])\n",
    "    \n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_with_embeddings = df.copy()\n",
    "    \n",
    "    # Track progress with tqdm\n",
    "    total_embeddings = len(df) * len(text_columns)\n",
    "    progress_bar = tqdm(total=total_embeddings, desc=\"Generating embeddings\")\n",
    "    \n",
    "    # Generate embeddings for each specified column\n",
    "    for col in text_columns:\n",
    "        # Create a new column name for the embeddings\n",
    "        embedding_col_name = f\"{col}_embedding\"\n",
    "        \n",
    "        # Initialize the embedding column with empty lists\n",
    "        df_with_embeddings[embedding_col_name] = [[] for _ in range(len(df))]\n",
    "        \n",
    "        # Process each row\n",
    "        for idx, row in df.iterrows():\n",
    "            # Skip empty or NaN text\n",
    "            if pd.isna(row[col]) or row[col] == \"\":\n",
    "                progress_bar.update(1)\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # Convert to string and generate embedding\n",
    "                text = str(row[col])\n",
    "                response = client.embeddings.create(\n",
    "                    input=text,\n",
    "                    model=model\n",
    "                )\n",
    "                embedding = response.data[0].embedding\n",
    "                \n",
    "                # Store the embedding in the DataFrame\n",
    "                df_with_embeddings.at[idx, embedding_col_name] = embedding\n",
    "                \n",
    "                # Rate limiting to avoid API throttling\n",
    "                #time.sleep(0.01)  # Reduced sleep time for the smaller model\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error generating embedding for row {idx}, column {col}: {str(e)}\")\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.update(1)\n",
    "    \n",
    "    progress_bar.close()\n",
    "    return df_with_embeddings\n",
    "\n",
    "# Generate embeddings using the specified model\n",
    "print(\"Starting embedding generation...\")\n",
    "df_with_embeddings = generate_embeddings(df, model=\"text-embedding-3-small\")\n",
    "df = df_with_embeddings  # Update the main dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed data with embeddings to the same subfolder as the original data\n",
    "output_path = os.path.join(DATA_DIR, DATA_SUBFOLDER, \"processed_data.pkl\")\n",
    "\n",
    "# Save as pickle for an efficient binary format that preserves all Python objects exactly as they are\n",
    "df.to_pickle(output_path)\n",
    "print(f\"Saved processed data with embeddings to: {output_path}\")\n",
    "\n",
    "print(\"Data preparation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verification of Final Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a random vector embedding sample from English and Chinese\n",
    "import random\n",
    "\n",
    "# Check if we have any English embeddings\n",
    "english_embedding_col = 'english_text_embedding'\n",
    "if english_embedding_col in df.columns and len(df) > 0:\n",
    "    # Get non-empty English embeddings\n",
    "    non_empty_english = df[df[english_embedding_col].apply(lambda x: isinstance(x, list) and len(x) > 0)]\n",
    "    if len(non_empty_english) > 0:\n",
    "        # Select a random row\n",
    "        random_row = random.choice(range(len(non_empty_english)))\n",
    "        random_idx = non_empty_english.index[random_row]\n",
    "        \n",
    "        # Get the embedding vector\n",
    "        english_embedding = df.at[random_idx, english_embedding_col]\n",
    "        \n",
    "        # Print some sample info\n",
    "        print(\"\\nRandom English Embedding Sample:\")\n",
    "        print(f\"Row index: {random_idx}\")\n",
    "        print(f\"Vector length: {len(english_embedding)}\")\n",
    "        print(f\"First 5 dimensions: {english_embedding[:5]}\")\n",
    "        print(f\"Last 5 dimensions: {english_embedding[-5:]}\")\n",
    "    else:\n",
    "        print(\"\\nNo non-empty English embeddings found.\")\n",
    "else:\n",
    "    print(f\"\\nNo '{english_embedding_col}' column found in the DataFrame.\")\n",
    "\n",
    "# Check if we have any Chinese embeddings\n",
    "chinese_embedding_col = 'chinese_text_embedding'\n",
    "if chinese_embedding_col in df.columns and len(df) > 0:\n",
    "    # Get non-empty Chinese embeddings\n",
    "    non_empty_chinese = df[df[chinese_embedding_col].apply(lambda x: isinstance(x, list) and len(x) > 0)]\n",
    "    if len(non_empty_chinese) > 0:\n",
    "        # Select a random row\n",
    "        random_row = random.choice(range(len(non_empty_chinese)))\n",
    "        random_idx = non_empty_chinese.index[random_row]\n",
    "        \n",
    "        # Get the embedding vector\n",
    "        chinese_embedding = df.at[random_idx, chinese_embedding_col]\n",
    "        \n",
    "        # Print some sample info\n",
    "        print(\"\\nRandom Chinese Embedding Sample:\")\n",
    "        print(f\"Row index: {random_idx}\")\n",
    "        print(f\"Vector length: {len(chinese_embedding)}\")\n",
    "        print(f\"First 5 dimensions: {chinese_embedding[:5]}\")\n",
    "        print(f\"Last 5 dimensions: {chinese_embedding[-5:]}\")\n",
    "    else:\n",
    "        print(\"\\nNo non-empty Chinese embeddings found.\")\n",
    "else:\n",
    "    print(f\"\\nNo '{chinese_embedding_col}' column found in the DataFrame.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
