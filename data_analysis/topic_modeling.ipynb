{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import kaleido\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from wordcloud import WordCloud\n",
    "import json\n",
    "from itables import show\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up directory paths dynamically\n",
    "current_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "PROJECT_ROOT = os.path.dirname(current_dir)\n",
    "\n",
    "# Set paths\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\", \"data2\")\n",
    "PICKLE_PATH = os.path.join(DATA_DIR, \"processed_data.pkl\")\n",
    "VIZ_DIR = os.path.join(DATA_DIR, \"visualizations\")\n",
    "\n",
    "# Create visualization directory if it doesn't exist\n",
    "os.makedirs(VIZ_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data directly from the pickle file\n",
    "print(\"Loading data from pickle file...\")\n",
    "df = pd.read_pickle(PICKLE_PATH)\n",
    "print(f\"Loaded dataframe with {len(df)} rows and {len(df.columns)} columns\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract English text and embeddings\n",
    "english_text_col = 'english_text'  # Assuming this is the column name\n",
    "english_embedding_col = 'english_text_embedding'  # Assuming this is the column name\n",
    "\n",
    "# Extract English text and filter out empty texts\n",
    "english_corpus = df[english_text_col].fillna(\"\").tolist()\n",
    "non_empty_indices = [i for i, doc in enumerate(english_corpus) if isinstance(doc, str) and doc.strip() != \"\"]\n",
    "filtered_corpus = [english_corpus[i] for i in non_empty_indices]\n",
    "print(f\"Filtered corpus contains {len(filtered_corpus)} non-empty documents\")\n",
    "\n",
    "# Extract pre-computed embeddings directly\n",
    "raw_embeddings = df[english_embedding_col].tolist()\n",
    "embeddings = np.array([raw_embeddings[i] for i in non_empty_indices])\n",
    "print(f\"Extracted embeddings with shape {embeddings.shape}\")\n",
    "\n",
    "filtered_technique = [df['technique'][i] for i in non_empty_indices]\n",
    "filtered_intent = [df['intent'][i] for i in non_empty_indices]\n",
    "filtered_success = [df['success'][i] for i in non_empty_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HDBSCAN with prediction_data=True\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=3,\n",
    "    min_samples=2,\n",
    "    cluster_selection_method='eom',\n",
    "    prediction_data=True  # Required for probability calculation\n",
    ")\n",
    "\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Create BERTopic model\n",
    "print(\"\\nCreating and fitting BERTopic model...\")\n",
    "topic_model = BERTopic(\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Fit the model using precomputed embeddings\n",
    "topics, probs = topic_model.fit_transform(filtered_corpus, embeddings=embeddings)\n",
    "\n",
    "# Get topic info\n",
    "topic_info = topic_model.get_topic_info()\n",
    "print(\"\\nTop 10 topics by document count:\")\n",
    "topic_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_model.visualize_distribution(probs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store all figures for later rendering as JPG\n",
    "figures_dict = {}\n",
    "print(\"\\nGenerating visualizations...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Topic barchart\n",
    "print(\"\\n1. Topic Barchart\")\n",
    "fig_barchart = topic_model.visualize_barchart(top_n_topics=10, n_words=10)\n",
    "# Resize the figure\n",
    "fig_barchart.update_layout(\n",
    "    width=800,  # Set width in pixels\n",
    "    height=500,  # Set height in pixels\n",
    ")\n",
    "figures_dict[\"english_topic_barchart\"] = fig_barchart\n",
    "fig_barchart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Topic hierarchy\n",
    "print(\"\\n2. Topic Hierarchy\")\n",
    "fig_hierarchy = topic_model.visualize_hierarchy()\n",
    "# Resize the figure\n",
    "fig_hierarchy.update_layout(\n",
    "    width=800,\n",
    "    height=600,\n",
    ")\n",
    "figures_dict[\"english_topic_hierarchy\"] = fig_hierarchy\n",
    "fig_hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Document visualization\n",
    "print(\"\\n3. Document Visualization\")\n",
    "# Create a simple 2D UMAP projection first\n",
    "reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "# Visualize documents using the pre-computed projection\n",
    "doc_viz = topic_model.visualize_documents(filtered_corpus, reduced_embeddings=reduced_embeddings)\n",
    "\n",
    "# Adjust the height and width\n",
    "doc_viz.update_layout(\n",
    "    width=800,  # Set width in pixels\n",
    "    height=600,  # Set height in pixels\n",
    ")\n",
    "\n",
    "figures_dict[\"english_document_visualization\"] = doc_viz\n",
    "doc_viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Visualize Topics per Technique \n",
    "print(\"\\n5. Topics per Technique Visualization\")\n",
    "topics_per_technique = topic_model.topics_per_class(filtered_corpus, classes=filtered_technique)\n",
    "\n",
    "# Get the figure\n",
    "fig_technique = topic_model.visualize_topics_per_class(topics_per_technique)\n",
    "\n",
    "# Update the layout with customized settings\n",
    "fig_technique.update_layout(\n",
    "    # Change y-axis title to \"Technique\"\n",
    "    yaxis_title=\"Technique\",\n",
    "    \n",
    "    # Update title to \"Topics by Intent\" with larger font and centered\n",
    "    title={\n",
    "        'text': \"Topics by Technique\",\n",
    "        'font': {'size': 24},\n",
    "        'x': 0.5,  # Center the title (0.5 is the center of the x-axis)\n",
    "        'xanchor': 'center'\n",
    "    },\n",
    "    \n",
    "    # Increase font sizes for all text elements\n",
    "    font=dict(size=14),\n",
    "    \n",
    "    # Increase axis title font sizes\n",
    "    xaxis_title_font=dict(size=18),\n",
    "    yaxis_title_font=dict(size=22),\n",
    "    \n",
    "    # Set specific width and height\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "# Store the updated figure in the dictionary\n",
    "figures_dict[\"english_topics_per_technique\"] = fig_technique\n",
    "\n",
    "# Display the figure\n",
    "fig_technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Visualize Topics per Intent\n",
    "print(\"\\n5. Topics per Intent Visualization\")\n",
    "topics_per_intent = topic_model.topics_per_class(filtered_corpus, classes=filtered_intent)\n",
    "\n",
    "# Get the figure\n",
    "fig_intent = topic_model.visualize_topics_per_class(topics_per_intent)\n",
    "\n",
    "# Update the layout with customized settings\n",
    "fig_intent.update_layout(\n",
    "    # Change y-axis title to \"Intent\"\n",
    "    yaxis_title=\"Intent\",\n",
    "    \n",
    "    # Update title to \"Topics by Intent\" with larger font and centered\n",
    "    title={\n",
    "        'text': \"Topics by Intent\",\n",
    "        'font': {'size': 24},\n",
    "        'x': 0.5,  # Center the title (0.5 is the center of the x-axis)\n",
    "        'xanchor': 'center'\n",
    "    },\n",
    "    \n",
    "    # Increase font sizes for all text elements\n",
    "    font=dict(size=14),\n",
    "    \n",
    "    # Increase axis title font sizes\n",
    "    xaxis_title_font=dict(size=18),\n",
    "    yaxis_title_font=dict(size=22),\n",
    "    \n",
    "    # Explicitly set width and height dimensions\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "# Store the updated figure in the dictionary\n",
    "figures_dict[\"english_topics_per_intent\"] = fig_intent\n",
    "\n",
    "# Display the figure\n",
    "fig_intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Topics per Success\n",
    "print(\"Topics per Success Visualization\") \n",
    "topics_per_success = topic_model.topics_per_class(filtered_corpus, classes=filtered_success)\n",
    "\n",
    "# Get the figure\n",
    "fig_success = topic_model.visualize_topics_per_class(topics_per_success)\n",
    "\n",
    "# Update the layout with customized settings\n",
    "fig_success.update_layout(\n",
    "    yaxis_title=\"Successful Prompt Injection Attack (True or False)\",\n",
    "    \n",
    "    # Update title to \"Topics by Intent\" with larger font and centered\n",
    "    title={\n",
    "        'text': \"Topics by Successful Prompt Injection Attack\",\n",
    "        'font': {'size': 22},\n",
    "        'x': 0.5,  # Center the title (0.5 is the center of the x-axis)\n",
    "        'xanchor': 'center'\n",
    "    },\n",
    "    \n",
    "    # Increase font sizes for all text elements\n",
    "    font=dict(size=14),\n",
    "    \n",
    "    # Increase axis title font sizes\n",
    "    xaxis_title_font=dict(size=18),\n",
    "    yaxis_title_font=dict(size=18),\n",
    "    \n",
    "    # Set specific width and height\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "# Store the updated figure in the dictionary\n",
    "figures_dict[\"english_topics_per_success\"] = fig_success\n",
    "\n",
    "# Display the figure\n",
    "fig_success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Topic Word Clouds\n",
    "print(\"\\n6. Topic Word Clouds\")\n",
    "# Create a directory for word clouds\n",
    "wordcloud_dir = os.path.join(VIZ_DIR, \"english\", \"wordclouds\")\n",
    "os.makedirs(wordcloud_dir, exist_ok=True)\n",
    "\n",
    "# Get top topics (including -1, no custom sorting needed)\n",
    "top_topics = topic_info['Topic'].head(10).tolist()\n",
    "\n",
    "# Generate word cloud for each top topic\n",
    "for topic in top_topics:\n",
    "    words = topic_model.get_topic(topic)\n",
    "    if words:\n",
    "        # Create a dictionary of word:weight pairs\n",
    "        word_dict = {word: weight for word, weight in words}\n",
    "        \n",
    "        # Generate word cloud\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white', \n",
    "                             colormap='viridis', max_words=50)\n",
    "        wordcloud.generate_from_frequencies(word_dict)\n",
    "        \n",
    "        # Create figure\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Topic {topic} Word Cloud')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save figure\n",
    "        plt.savefig(os.path.join(wordcloud_dir, f\"topic_{topic}_wordcloud.png\"))\n",
    "        \n",
    "        # Display in VSCode notebook\n",
    "        # plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "print(\"\\n7. Corpus-Based Word Cloud\")\n",
    "\n",
    "# Combine all documents into one text\n",
    "all_text = \" \".join(filtered_corpus)\n",
    "\n",
    "# Split on whitespace since the text is already preprocessed\n",
    "tokens = all_text.split()\n",
    "\n",
    "# Count token frequencies\n",
    "word_freq = Counter(tokens)\n",
    "\n",
    "# Get English stopwords\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stopwords from word_freq dictionary\n",
    "filtered_word_freq = {word: freq for word, freq in word_freq.items() if word.lower() not in english_stopwords}\n",
    "\n",
    "# Generate corpus-based wordcloud with filtered frequencies\n",
    "corpus_wordcloud = WordCloud(width=1000, height=500, background_color='white', \n",
    "                          colormap='plasma', max_words=50)\n",
    "corpus_wordcloud.generate_from_frequencies(filtered_word_freq)\n",
    "\n",
    "# Create and display corpus-based wordcloud\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(corpus_wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud Based on Full Corpus (Stopwords Removed)')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save corpus-based wordcloud\n",
    "plt.savefig(os.path.join(wordcloud_dir, \"corpus_wordcloud_no_stopwords.png\"))\n",
    "\n",
    "# Display in VSCode notebook\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotly_dir = os.path.join(VIZ_DIR, \"english\", \"plotly\")\n",
    "# os.makedirs(plotly_dir, exist_ok=True)\n",
    "\n",
    "# # Save all figures in the dictionary\n",
    "# for fig_name, fig in figures_dict.items():\n",
    "#     fig.write_image(os.path.join(plotly_dir, f\"{fig_name}.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chinese Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['chinese_text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Chinese text and embeddings\n",
    "chinese_text_col = 'chinese_text'  # Assuming this is the column name\n",
    "chinese_embedding_col = 'chinese_text_embedding'  # Assuming this is the column name\n",
    "\n",
    "# Extract Chinese text and filter out empty texts\n",
    "chinese_corpus = df[chinese_text_col].fillna(\"\").tolist()\n",
    "non_empty_indices = [i for i, doc in enumerate(chinese_corpus) if doc.strip() != \"\"]\n",
    "filtered_corpus = [chinese_corpus[i] for i in non_empty_indices]\n",
    "print(f\"Filtered corpus contains {len(filtered_corpus)} non-empty documents\")\n",
    "\n",
    "# Extract pre-computed embeddings directly\n",
    "raw_embeddings = df[chinese_embedding_col].tolist()\n",
    "embeddings = np.array([raw_embeddings[i] for i in non_empty_indices])\n",
    "print(f\"Extracted embeddings with shape {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "def tokenize_zh(text):\n",
    "    words = jieba.lcut(text)\n",
    "    return words\n",
    "\n",
    "vectorizer_model = CountVectorizer(tokenizer=tokenize_zh)\n",
    "\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=8,\n",
    "    min_samples=5,\n",
    "    cluster_selection_method='leaf',\n",
    "    prediction_data=True  \n",
    ")\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(filtered_corpus, embeddings=embeddings)\n",
    "\n",
    "topic_info = topic_model.get_topic_info()\n",
    "print(\"\\nTop 10 topics by document count:\")\n",
    "topic_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Topic barchart\n",
    "print(\"\\n1. Topic Barchart\")\n",
    "fig_barchart = topic_model.visualize_barchart(top_n_topics=10, n_words=10)\n",
    "figures_dict[\"chinese_topic_barchart\"] = fig_barchart\n",
    "fig_barchart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Topic hierarchy\n",
    "print(\"\\n2. Topic Hierarchy\")\n",
    "fig_hierarchy = topic_model.visualize_hierarchy(top_n_topics=50)\n",
    "figures_dict[\"chinese_topic_hierarchy\"] = fig_hierarchy\n",
    "fig_hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Document visualization\n",
    "print(\"\\n3. Document Visualization\")\n",
    "# Create a simple 2D UMAP projection first\n",
    "reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "# Visualize documents using the pre-computed projection\n",
    "doc_viz = topic_model.visualize_documents(filtered_corpus, reduced_embeddings=reduced_embeddings)\n",
    "figures_dict[\"chinese_document_visualization\"] = doc_viz\n",
    "doc_viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Chinese Topic Word Clouds\n",
    "print(\"\\n4. Chinese Topic Word Clouds\")\n",
    "# Create a directory for word clouds\n",
    "wordcloud_dir = os.path.join(VIZ_DIR, \"chinese\", \"wordclouds\")\n",
    "os.makedirs(wordcloud_dir, exist_ok=True)\n",
    "\n",
    "# Get top topics (including -1, no custom sorting needed)\n",
    "top_topics = topic_info['Topic'].head(10).tolist()\n",
    "\n",
    "# Generate word cloud for each top topic\n",
    "for topic in top_topics:\n",
    "    words = topic_model.get_topic(topic)\n",
    "    if words:\n",
    "        # Create a dictionary of word:weight pairs\n",
    "        word_dict = {word: weight for word, weight in words}\n",
    "        \n",
    "        # Generate word cloud\n",
    "        # Note: For Chinese text, we need a font that supports Chinese characters\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white', \n",
    "                             colormap='viridis', max_words=50) \n",
    "        wordcloud.generate_from_frequencies(word_dict)\n",
    "        \n",
    "        # Create figure\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Chinese Topic {topic} Word Cloud')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save figure\n",
    "        plt.savefig(os.path.join(wordcloud_dir, f\"topic_{topic}_wordcloud.png\"))\n",
    "        \n",
    "        # Display in VSCode notebook\n",
    "        # plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process chinese text using jieba tokenizer\n",
    "def prepare_chinese_corpus(texts):\n",
    "    \"\"\"\n",
    "    Prepare Chinese corpus for topic modeling by tokenizing and filtering\n",
    "    \"\"\"\n",
    "    processed_texts = []\n",
    "    for text in texts:\n",
    "        if isinstance(text, str) and text.strip():\n",
    "            # Tokenize using the provided function\n",
    "            tokens = tokenize_zh(text)\n",
    "            \n",
    "            # Filter out stopwords if you have a stopwords list\n",
    "            # tokens = [word for word in tokens if word not in chinese_stopwords]\n",
    "            \n",
    "            # Filter out single characters (often not meaningful in Chinese)\n",
    "            tokens = [word for word in tokens if len(word) > 1]\n",
    "            \n",
    "            # Join back for BERTopic or keep as tokens depending on your embedding method\n",
    "            processed_text = ' '.join(tokens)  # For some embeddings, space-separated is fine\n",
    "            processed_texts.append(processed_text)\n",
    "    \n",
    "    return processed_texts\n",
    "\n",
    "processed_chinese_corpus = prepare_chinese_corpus(chinese_corpus)\n",
    "# print(processed_chinese_corpus[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n5. Chinese Corpus-Based Word Cloud\")\n",
    "\n",
    "all_tokens = []\n",
    "for text in processed_chinese_corpus:\n",
    "    tokens = text.split()\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "word_freq = Counter(all_tokens)\n",
    "\n",
    "font_path = '/Library/Fonts/Arial Unicode.ttf'  \n",
    "\n",
    "# Generate the word cloud with Chinese font\n",
    "corpus_wordcloud = WordCloud(\n",
    "    width=1000, \n",
    "    height=500, \n",
    "    background_color='white',\n",
    "    colormap='plasma', \n",
    "    max_words=50,\n",
    "    font_path=font_path  # Important for Chinese characters\n",
    ")\n",
    "corpus_wordcloud.generate_from_frequencies(word_freq)\n",
    "\n",
    "# Create and display corpus-based wordcloud\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(corpus_wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud Based on Chinese Corpus')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save corpus-based wordcloud\n",
    "plt.savefig(os.path.join(wordcloud_dir, \"corpus_wordcloud.png\"))\n",
    "\n",
    "# Display in VSCode notebook\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save all plotly figures using kaleido (currently not working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(\"\\nSaving all Plotly figures as JPG...\")\n",
    "# print(figures_dict.keys())\n",
    "# print(len(figures_dict), \"figures to save\")\n",
    "# for fig_name, fig in figures_dict.items():\n",
    "#     jpg_path = os.path.join(VIZ_DIR, f\"{fig_name}.png\")\n",
    "#     print(jpg_path)\n",
    "#     fig.write_image(jpg_path)\n",
    "#     print(f\"Saved {jpg_path}\")\n",
    "\n",
    "# print(\"\\nTopic modeling complete!\")\n",
    "# print(f\"Visualizations saved to: {VIZ_DIR}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
